{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "07dc92d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "081272ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Schimatári' 'Firá' 'Mantoúdi' 'Lixoúri' 'Perivóli' 'Tzermiádo' 'Níkaia'\n",
      " 'Mýkonos' 'Skýros' 'Áno Kalentíni' 'Néa Výssa' 'Voúla' 'Kallithéa'\n",
      " 'Smínthi' 'Kremastí' 'Ierissós' 'Eleónas' 'Préveza' 'Alistráti'\n",
      " 'Igoumenítsa' 'Ampelákia' 'Síkinos' 'Kos' 'Argalastí' 'Galátista'\n",
      " 'Ligourió' 'Míthymna' 'Pýrgos' 'Pérama' 'Chiliomódi' 'Kontokáli' 'Mália'\n",
      " 'Kýthnos' 'Palaiokómi' 'Anatolikó' 'Loutráki' 'Skýdra' 'Antimácheia'\n",
      " 'Amfilochía' 'Antíparos' 'Megísti' 'Xylókastro' 'Makrakómi' 'Isthmía'\n",
      " 'Thessaloníki' 'Kímolos' 'Chaïdári' 'Áno Sýros' 'Káto Goúves' 'Orestiáda']\n",
      "[[38.35   23.5833]\n",
      " [36.4102 25.4056]\n",
      " [38.7981 23.4797]\n",
      " [38.2    20.4333]\n",
      " [39.4208 20.0139]\n",
      " [35.1983 25.49  ]\n",
      " [39.5667 22.4667]\n",
      " [37.4453 25.3287]\n",
      " [38.9064 24.5658]\n",
      " [39.2515 21.1679]\n",
      " [41.5833 26.5333]\n",
      " [37.8459 23.7626]\n",
      " [40.275  22.575 ]\n",
      " [41.2289 24.8804]\n",
      " [36.4105 28.1191]\n",
      " [40.3983 23.8783]\n",
      " [38.3633 23.4428]\n",
      " [38.95   20.75  ]\n",
      " [41.0633 23.9581]\n",
      " [39.5034 20.2673]\n",
      " [37.95   23.5167]\n",
      " [36.6833 25.1167]\n",
      " [36.8153 27.1103]\n",
      " [39.2257 23.2197]\n",
      " [40.4672 23.2806]\n",
      " [37.6144 23.0361]\n",
      " [39.3686 26.1806]\n",
      " [37.6667 21.4333]\n",
      " [35.3695 24.7039]\n",
      " [37.8139 22.8711]\n",
      " [39.646  19.85  ]\n",
      " [35.2836 25.4625]\n",
      " [37.3833 24.4167]\n",
      " [40.8714 23.9017]\n",
      " [40.6631 22.7114]\n",
      " [37.975  22.9798]\n",
      " [40.7667 22.15  ]\n",
      " [36.8125 27.0983]\n",
      " [38.8592 21.1758]\n",
      " [37.0404 25.0819]\n",
      " [36.145  29.585 ]\n",
      " [38.0667 22.6333]\n",
      " [38.9333 22.1167]\n",
      " [37.9155 23.0073]\n",
      " [40.6403 22.9356]\n",
      " [36.8    24.55  ]\n",
      " [38.0167 23.65  ]\n",
      " [37.45   24.9   ]\n",
      " [35.33   25.3119]\n",
      " [41.5    26.5333]]\n"
     ]
    }
   ],
   "source": [
    "import db1\n",
    "import ApoKomb1    \n",
    "import torch \n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "citys = db1.basi()\n",
    "\n",
    "citys1 = citys.sample(50)\n",
    "lats_c = citys1[[\"lat\"]]\n",
    "lngs_c = citys1[[\"lng\"]]\n",
    "node_citys =  np.array(citys1[\"city\"])\n",
    "\n",
    "location = citys1[[\"lat\", \"lng\"]].astype(float)\n",
    "#edw xrisimopoioume ena DataFrame gia na mporoume na exoume to ka8e edge se sigkekrimeno ar8imo gia na\n",
    "#mhn xa8oume sthn sunexia apo pio polu paei se pia \n",
    "\n",
    "\n",
    "unique_node_citys= citys1[\"city\"]\n",
    "unique_node_citys = pd.DataFrame(data={\n",
    "    'city' : unique_node_citys,\n",
    "    'mappedId': pd.RangeIndex(len(unique_node_citys))\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "coords = ApoKomb1.sindet(lats_c, lngs_c)\n",
    "edge_index = ApoKomb1.edge_index(node_citys)\n",
    "distances = ApoKomb1.apostasis(node_citys, coords)\n",
    "\n",
    "print(node_citys)\n",
    "print(coords)\n",
    "#print(edge_index)\n",
    "#print(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "353cd237",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0,    0,    0,  ..., 1218, 1219, 1220],\n",
      "        [   1,  652,  379,  ..., 1224, 1223, 1221]])\n",
      "Original Graph:  Graph with 50 nodes and 1225 edges\n",
      "Line Graph:  Graph with 1225 nodes and 58800 edges\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import numpy as mp\n",
    "#from haversine import haversine\n",
    "\n",
    "#sto sigkekrimeno meros tou kwdika pernoume ta dedomena apo prin kai ta topo8etoume se enan algorithmo gia na \n",
    "#broume tis apostaseis apo ka8e kombo, sth synexia ta bazoume ta apotelesmata os (EDGE FEATURES) kai meta\n",
    "#Ftiaxnoume ena grafima me auta kai sth sunexia to metatrepoume se gramiko grafima gia na exoume ta (EDGE FEATURES) os (NODE FEATURES)\n",
    "\n",
    "# Example usage:\n",
    "# Create a sample graph\n",
    "G = nx.Graph()\n",
    "for i in range(len(edge_index)):\n",
    "    G.add_edge(edge_index[i][0], edge_index[i][1], weight=distances[i])\n",
    "\n",
    "def line_graph(G):\n",
    "    L = nx.line_graph(G)\n",
    "    for node in L.nodes:\n",
    "        edges = G.edges(node)\n",
    "        if edges:\n",
    "            edge_features = G.edges[node]['weight']\n",
    "            L.nodes[node]['e_f'] = edge_features\n",
    "        else:\n",
    "            L.nodes[node]['e_f'] = None\n",
    "    return L\n",
    "\n",
    "def line_graph_node_features(G):\n",
    "    node_features = []\n",
    "    for _, node_data in G.nodes(data=True):\n",
    "        node_features.append(node_data['e_f'])\n",
    "    return node_features\n",
    "\n",
    "L = line_graph(G)\n",
    "node_features1 = line_graph_node_features(L)\n",
    "\n",
    "# Create a mapping from original graph edges to line graph nodes\n",
    "edge_to_index = {edge: idx for idx, edge in enumerate(L.nodes())}\n",
    "\n",
    "# Prepare edge index for the line graph\n",
    "edge_index_t = []\n",
    "for edge in L.edges:\n",
    "    src, dst = edge\n",
    "    edge_index_t.append([edge_to_index[src], edge_to_index[dst]])\n",
    "\n",
    "# Convert edge list to a 2D tensor\n",
    "edge_index_lg = torch.tensor(edge_index_t, dtype=torch.long).t().contiguous()\n",
    "print(edge_index_lg)\n",
    "# Convert node features to tensor\n",
    "#nf = torch.tensor(node_features1, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "\n",
    "#nx.draw(G, with_labels= True)\n",
    "# Transform the graph into its line graph representation\n",
    "print('Original Graph: ',G)\n",
    "\n",
    "\n",
    "#nx.draw(L, with_labels= True)\n",
    "print('Line Graph: ',L)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "912f9623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: tensor([[ 11.7973],\n",
      "        [-17.3433],\n",
      "        [-13.6178],\n",
      "        ...,\n",
      "        [-22.3886],\n",
      "        [  1.7854],\n",
      "        [ 11.8960]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import networkx as nx\n",
    "\n",
    "#edw dimiourgiete to nevroniko diktio to opoio exei eksis kommatia \n",
    "#1) To EmbeddingLayer to opoio exei os apotelesma sto forward (h0ij = Wxij + b,)\n",
    "#2) Ena Graph Attection Layer ftMHA(htij , L(G))\n",
    "#3) Ena Feed Forward Layer  ftFF(h˙t+1ij)\n",
    "#4) Ena Output Layer to opoio meta apo epeksergasia apo ola ta alla layers bgazei to regret sto forward rˆij = WhTij + b\n",
    "#5) Ola mazi ftiaxnoun to nevroniko diktuo \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_layers=1):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.W = nn.Parameter(torch.randn(input_dim, output_dim))  # Learnable weight matrix\n",
    "        self.b = nn.Parameter(torch.zeros(output_dim))  # Learnable biases\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        for _ in range(self.num_layers):\n",
    "            h = torch.matmul(h, self.W)\n",
    "            h += self.b\n",
    "        return h\n",
    "\n",
    "class GATLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_heads=8, concat=True):\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.gat = GATConv(input_dim, output_dim, heads=num_heads, concat=concat, dropout=0.6)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        #print(x)\n",
    "        x = self.gat(x, edge_index)\n",
    "        return F.elu(x)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, input_size, output_size=512):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class Output_layer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Output_layer, self).__init__()\n",
    "        self.W = nn.Parameter(torch.randn(input_dim, output_dim))  # Learnable weight matrix\n",
    "        self.b = nn.Parameter(torch.zeros(output_dim))  # Learnable biases\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0ij = torch.matmul(x, self.W)  # Assuming x has shape (batch_size, num_edges, input_dim)\n",
    "        h0ij += self.b\n",
    "        return h0ij\n",
    "    \n",
    "class GNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.embedding_layer = EmbeddingLayer(input_dim, hidden_dim)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.gat_layers = nn.ModuleList([GATLayer(hidden_dim, hidden_dim // 8, num_heads=8) for _ in range(num_layers)])\n",
    "        self.feedforward = FeedForward(hidden_dim, hidden_dim)  # Adjusted output_size\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_dim)\n",
    "        self.output_layer = Output_layer(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index1):\n",
    "        x1 = self.embedding_layer(x)\n",
    "        #print(\"After embedding layer:\", x1.shape)\n",
    "        #print(\"After embedding layer:\", edge_index1)\n",
    "        for layer in self.gat_layers:\n",
    "            x2 = layer(x1, edge_index1)\n",
    "            #print(\"After GAT layer:\", x2.shape)\n",
    "            x1 = x1 + x2\n",
    "\n",
    "        combined1 = x1\n",
    "        bn_output1 = self.batch_norm(combined1)\n",
    "       # print(\"After first batch norm:\", bn_output1.shape)\n",
    "        \n",
    "        # Apply feedforward operation\n",
    "        ff_output = self.feedforward(bn_output1)\n",
    "        #print(\"After feedforward layer:\", ff_output.shape)\n",
    "        \n",
    "        # Add the original input with the feedforward output\n",
    "        combined = bn_output1 + ff_output\n",
    "       # print(\"After adding feedforward output:\", combined.shape)\n",
    "        \n",
    "        # Apply batch normalization\n",
    "        bn_output = self.batch_norm(combined)\n",
    "        #print(\"After second batch norm:\", bn_output.shape)\n",
    "        \n",
    "        x = self.output_layer(bn_output)\n",
    "        #print(\"After output layer:\", x.shape)\n",
    "        return x\n",
    "    \n",
    "# Define model parameters\n",
    "dx = 1  # Dimension of edge features\n",
    "dh = 120  # Dimension of hidden layers\n",
    "output_dim = 1  # Output dimension\n",
    "num_layers = 5 # Number of GNN layers\n",
    "\n",
    "nf = torch.tensor(node_features1, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "\n",
    "#adjacency_matrix = torch.tensor(nx.adjacency_matrix(L).todense(), dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Instantiate the GNN model\n",
    "model = GNNModel(dx, dh, output_dim, num_layers)\n",
    "#print(adjacency_matrix)\n",
    "# Forward pass\n",
    "\n",
    "#edge_index1= torch.tensor(edge_index, dtype=torch.long)\n",
    "\n",
    "\n",
    "#edge_index1 = edge_index1.t().contiguous()\n",
    "#print(edge_index1)\n",
    "output = model(nf, edge_index_lg) #adjacency_matrix, \n",
    "print(\"Output shape:\", output)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d55a87d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "374e47d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Output shape:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7cbb7f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_edge_index_and_regret(output,edge_index):\n",
    "    output = torch.squeeze(output, 1)\n",
    "    output1= output.detach().numpy()\n",
    "    node_index2 = []\n",
    "    node_index1 = []\n",
    "    node_index3 = []\n",
    "    regret_values = []\n",
    "    \n",
    "    \n",
    "    #nodes=[]\n",
    "    #for node in L.nodes(data=True):\n",
    "        #nodes.append(node[0])\n",
    "    for i in range(len(edge_index)):\n",
    "        node_index3.append(edge_index[i])\n",
    "        regret_values.append(output1[i])\n",
    "\n",
    "        \n",
    "    for i in range(len(edge_index)):\n",
    "        node_index2.append(edge_index[i][1])\n",
    "        node_index1.append(edge_index[i][0])\n",
    "        node_index3.append((node_index2[i], node_index1[i]))\n",
    "        regret_values.append(output1[i])\n",
    "        \n",
    "   \n",
    "    return node_index3,regret_values\n",
    "\n",
    "def find_the_value(edges,nodes):\n",
    "    return nodes.index((edges[0],edges[1]))\n",
    "\n",
    "node_index3,regret_values = make_edge_index_and_regret(output,edge_index)\n",
    "#print(node_index3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3cf037ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relocate_algorithm: [0, 46, 11, 32, 47, 7, 39, 45, 21, 1, 28, 48, 31, 5, 22, 37, 14, 40, 49, 10, 13, 15, 18, 33, 24, 44, 36, 34, 12, 6, 23, 2, 26, 8, 42, 9, 19, 30, 4, 17, 38, 3, 27, 41, 29, 25, 43, 35, 20, 16] 4078.7614407493247\n",
      "Relocated node: 23 New Distance: 3965.7577046147694\n",
      "Relocated node: 23 New Distance: 3928.7381427419796\n",
      "Relocated node: 23 New Distance: 3926.996124799503\n",
      "Best solution: [0, 46, 11, 32, 47, 7, 39, 45, 21, 1, 28, 48, 31, 5, 37, 22, 14, 40, 49, 10, 13, 18, 33, 15, 24, 44, 34, 36, 12, 6, 23, 26, 8, 2, 42, 9, 19, 30, 4, 17, 38, 3, 27, 41, 29, 25, 43, 35, 20, 16] 3926.996124799503\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "#edw einai mia apli me8odos get() gia na mporoume na xwrisoume apo (1,2) dedomeno se (1) kai (2)\n",
    "def get_nodes(edge):\n",
    "    return edge[0], edge[1]\n",
    "\n",
    "def edge_on_tour(edge, tour):\n",
    "    tour_edges = tour_to_table(tour)\n",
    "    return edge in tour_edges or (edge[1], edge[0]) in tour_edges\n",
    "\n",
    "#edw kanei ar8isi oles tis apostaseis pou epilegonte san diadromh gia na bgalei thn sunoliki apostash\n",
    "def calculate_distance(path, graph):\n",
    "    distance = 0\n",
    "    for i in range(len(path) - 1):\n",
    "        distance += graph[path[i]][path[i + 1]]\n",
    "    distance += graph[path[-1]][path[0]]  # Return to starting point\n",
    "    return distance\n",
    "\n",
    "#Edw exoume thn prwti me8odo pou einai Greedy Nearest Neighbor sigkekrimena epilegi me8odika tous kombous me thn seira pou\n",
    "#den exoume episkeftei kai episis exoun to mikrotero regret\n",
    "def greedy_nearest_neighbor(regret, edges, distances):\n",
    "    num_cities = distances.shape[0]\n",
    "    visited = [False] * num_cities\n",
    "    tour = [0]  # Start from city 0\n",
    "    current_city = 0\n",
    "    visited[current_city] = True\n",
    "    sum_tour_d = 0\n",
    "\n",
    "    for _ in range(num_cities - 1):\n",
    "        nearest_city = None\n",
    "        min_distance = float('inf')\n",
    "        \n",
    "        for next_city in range(len(regret)):\n",
    "            poli1, poli2 = get_nodes(edges[next_city])\n",
    "            if visited[poli1] == False and visited[poli2] == True and current_city == poli2:\n",
    "                if regret[next_city] < min_distance:\n",
    "                    nearest_city = poli1\n",
    "                    min_distance = regret[next_city]\n",
    "                    \n",
    "            elif visited[poli2] == False and visited[poli1] == True and current_city == poli1:\n",
    "                if regret[next_city] < min_distance:\n",
    "                    nearest_city = poli2\n",
    "                    min_distance = regret[next_city]\n",
    "\n",
    "        #sum_tour.append(min_distance)\n",
    "        tour.append(nearest_city)\n",
    "        visited[nearest_city] = True\n",
    "        current_city = nearest_city\n",
    "        sum_tour_d = calculate_distance(tour, distances1)\n",
    "\n",
    "    return tour, sum_tour_d  #sum_tour, \n",
    "\n",
    "#Se afti thn me8odo ksana enonoume ta dedomena, apo to na exoume enan pinaka [1,2,3,4] exoume [(1,2), (2,3), (3,4)]\n",
    "def tour_to_table(tour):\n",
    "    table = []\n",
    "    for i in range(len(tour) - 1):\n",
    "        pair = (tour[i], tour[i + 1])\n",
    "        table.append(pair)\n",
    "    return table\n",
    "\n",
    "def penalize_edges(regret_values, penalties, edges):\n",
    "    utilities = np.zeros(len(edges))\n",
    "    \n",
    "    for i in range(len(edges)):\n",
    "        edge = edges[i]\n",
    "        edge_idx = find_the_value(edge, node_index3)\n",
    "        utilities[i] = regret_values[edge_idx] / (1 + penalties[edge_idx])\n",
    "    \n",
    "    max_utility = np.max(utilities)\n",
    "    max_utility_indices = np.where(utilities == max_utility)[0]\n",
    "    max_utility_edges = [edges[i] for i in max_utility_indices]\n",
    "\n",
    "    for edge in max_utility_edges:\n",
    "        edge_idx = find_the_value(edge, node_index3)\n",
    "        penalties[edge_idx] += 1\n",
    "\n",
    "    return penalties, max_utility_edges\n",
    "#Auth h me8odos pernei dio ari8mous apo ton pinaka (dio kombous apo thn diadromi) kai tous allazei 8esi sth diadromi\n",
    "def two_opt_swap(path, i, j):\n",
    "    new_path = path[:i] + path[i:j+1][::-1] + path[j+1:]\n",
    "    return new_path\n",
    "\n",
    "#Edw exoume thn me8odo 2opt swap dld pernei dio kombous apo thn diadromi kai tous allazei metaksei tous \n",
    "#o algorithoms stamataei kai molis bre8ei kaluteri diadromo (mikroteri apostasi)\n",
    "def two_opt_local_search(path, graph, best_distance):\n",
    "    n = len(path)\n",
    "    best_path = path\n",
    "    improved = True\n",
    "\n",
    "    while improved:\n",
    "        improved = False\n",
    "        for i in range(1, n - 2):\n",
    "            for j in range(i + 1, n):\n",
    "                if j - i == 1:  # No point in reversing if i and j are adjacent\n",
    "                    continue\n",
    "                new_path = two_opt_swap(best_path, i, j)\n",
    "                new_distance = calculate_distance(new_path, graph)\n",
    "                \n",
    "                if new_distance < best_distance:\n",
    "                    best_distance = new_distance\n",
    "                    best_path = new_path\n",
    "                    improved = True\n",
    "                    break\n",
    "            if improved:\n",
    "                break\n",
    "\n",
    "    return best_path, best_distance\n",
    "\n",
    "#Afti i me8odos allazei 8esi mono se enan kombo sth diadromi gia na beltiosi thn diadromi \n",
    "def relocate_algorithm(distance_matrix, old_route, best_distance, max_iterations=1000):\n",
    "    num_cities = len(old_route)\n",
    "    current_route = old_route[:]  # Copy the initial route\n",
    "    current_distance = best_distance\n",
    "    \n",
    "    for _ in range(max_iterations):\n",
    "        city1, city2 = np.random.choice(num_cities, size=2, replace=False)\n",
    "        \n",
    "        # Ensure city1 is always before city2 in the tour\n",
    "        if city1 > city2:\n",
    "            city1, city2 = city2, city1\n",
    "            \n",
    "        new_route = current_route[:city1] + current_route[city1+1:]\n",
    "        new_route = new_route[:city2] + [current_route[city1]] + new_route[city2:]\n",
    "        new_distance = calculate_distance(new_route, distance_matrix)\n",
    "\n",
    "        if new_distance < current_distance:\n",
    "            current_route = new_route\n",
    "            current_distance = new_distance\n",
    "\n",
    "    return current_route, current_distance\n",
    "\n",
    "\n",
    "def local_search_on_penalized_edges(solution, best_distance, max_utility_edges, node_index3, regret_values, distances):\n",
    "    improved_tour = solution[:]\n",
    "    improved_dis = best_distance\n",
    "\n",
    "    #print(\"Initial Solution:\", improved_tour)\n",
    "    #print(\"Penalized Edges:\", max_utility_edges)\n",
    "\n",
    "    # Perform 2-opt swaps focusing on penalized edges\n",
    "    for edge in max_utility_edges:\n",
    "        #print(\"Trying to remove edge:\", edge, max_utility_edges)\n",
    "        for i in range(1, len(improved_tour) - 1):\n",
    "            for j in range(i + 1, len(improved_tour)):\n",
    "                if (improved_tour[i], improved_tour[j]) == edge or (improved_tour[j], improved_tour[i]) == edge:\n",
    "                    new_path = two_opt_swap(improved_tour, i, j)\n",
    "                    new_distance = calculate_distance(new_path, distances)\n",
    "                    #print(\"Old Distance:\", best_distance, \"New Distance:\", new_distance)\n",
    "                    if new_distance < improved_dis:\n",
    "                        improved_tour = new_path\n",
    "                        improved_dis = new_distance\n",
    "                        print(\"Removed edge with 2-opt:\", edge, \"New Distance:\", new_distance)\n",
    "\n",
    "    # Perform relocation focusing on penalized edges\n",
    "    for edge in max_utility_edges:\n",
    "        #print(\"Trying to relocate node in edge:\", edge)\n",
    "        for i in range(1, len(improved_tour) - 1):\n",
    "            if improved_tour[i] in edge:\n",
    "                for j in range(len(improved_tour)):\n",
    "                    if j != i and (improved_tour[j], improved_tour[i]) not in max_utility_edges and (improved_tour[i], improved_tour[j]) not in max_utility_edges:\n",
    "                        new_path, new_distance = relocate_algorithm(distances, improved_tour, improved_dis, improved_tour[i])\n",
    "                        #print(\"Old Distance:\", best_distance, \"New Distance:\", new_distance)\n",
    "                        if new_distance < improved_dis:\n",
    "                            improved_tour = new_path\n",
    "                            improved_dis = new_distance\n",
    "                            print(\"Relocated node:\", improved_tour[i], \"New Distance:\", new_distance)\n",
    "\n",
    "    return improved_tour, improved_dis\n",
    "\n",
    "\n",
    "def gls(distances, regret_values, node_index3, max_iterations=20):\n",
    "    initial_solution, initial_distance = greedy_nearest_neighbor(regret_values, node_index3, distances)\n",
    "    best_solution_t, best_distance_t = two_opt_local_search(initial_solution, distances, initial_distance)\n",
    "    best_solution, best_distance = relocate_algorithm(distances, best_solution_t, best_distance_t)\n",
    "    \n",
    "    #print(\"greedy_nearest_neighbor:\",initial_solution, initial_distance)\n",
    "    #print(\"two_opt_local_search:\",best_solution_t, best_distance_t)\n",
    "    print(\"relocate_algorithm:\",best_solution, best_distance)\n",
    "    \n",
    "    penalties = [0] * len(regret_values)\n",
    "    for _ in range(max_iterations):\n",
    "        edges = tour_to_table(best_solution)\n",
    "        penalties, max_utility_edges = penalize_edges(regret_values, penalties, edges)\n",
    "        #\n",
    "        best_solution, best_distance = local_search_on_penalized_edges(best_solution, best_distance, max_utility_edges, node_index3, regret_values, distances)\n",
    "    #print(penalties, max_utility_edges)\n",
    "    #print(node_index3)\n",
    "    return best_solution, best_distance\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    distances1 = ApoKomb1.apostasis_test(node_citys, coords)\n",
    "    \n",
    "    \n",
    "\n",
    "    best_solution, best_distance = gls(distances1, regret_values, node_index3)\n",
    "    print(\"Best solution:\", best_solution,best_distance)\n",
    "    #print(\"Best distance:\", best_distance)\n",
    "    #print(node_index3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "736a2f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H beltisti diadromi einai:\n",
      "Schimatári\n",
      "Chaïdári\n",
      "Voúla\n",
      "Kýthnos\n",
      "Áno Sýros\n",
      "Mýkonos\n",
      "Antíparos\n",
      "Kímolos\n",
      "Síkinos\n",
      "Firá\n",
      "Pérama\n",
      "Káto Goúves\n",
      "Mália\n",
      "Tzermiádo\n",
      "Antimácheia\n",
      "Kos\n",
      "Kremastí\n",
      "Megísti\n",
      "Orestiáda\n",
      "Néa Výssa\n",
      "Smínthi\n",
      "Alistráti\n",
      "Palaiokómi\n",
      "Ierissós\n",
      "Galátista\n",
      "Thessaloníki\n",
      "Anatolikó\n",
      "Skýdra\n",
      "Kallithéa\n",
      "Níkaia\n",
      "Argalastí\n",
      "Míthymna\n",
      "Skýros\n",
      "Mantoúdi\n",
      "Makrakómi\n",
      "Áno Kalentíni\n",
      "Igoumenítsa\n",
      "Kontokáli\n",
      "Perivóli\n",
      "Préveza\n",
      "Amfilochía\n",
      "Lixoúri\n",
      "Pýrgos\n",
      "Xylókastro\n",
      "Chiliomódi\n",
      "Ligourió\n",
      "Isthmía\n",
      "Loutráki\n",
      "Ampelákia\n",
      "Eleónas\n"
     ]
    }
   ],
   "source": [
    "print(\"H beltisti diadromi einai:\")\n",
    "for i in range(len(node_citys)):\n",
    "    print(node_citys[best_solution[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b24d63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb6697d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "58848da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --force python_tsp\n",
    "#!pip install --force nomba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fb64a8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 23, 12, 36, 34, 44, 24, 15, 33, 18, 13, 10, 49, 26, 8, 47, 7, 37, 22, 14, 40, 5, 31, 48, 28, 1, 21, 39, 45, 32, 11, 46, 20, 43, 29, 25, 27, 3, 38, 17, 4, 30, 19, 9, 6, 42, 41, 35, 16] 3771.4193297726056\n",
      "[0, 46, 11, 32, 47, 7, 39, 45, 21, 1, 28, 48, 31, 5, 37, 22, 14, 40, 49, 10, 13, 18, 33, 15, 24, 44, 34, 36, 12, 6, 23, 26, 8, 2, 42, 9, 19, 30, 4, 17, 38, 3, 27, 41, 29, 25, 43, 35, 20, 16] 3926.996124799503\n"
     ]
    }
   ],
   "source": [
    "#from python_tsp.exact import solve_tsp_dynamic_programming\n",
    "from python_tsp.heuristics import solve_tsp_local_search, solve_tsp_simulated_annealing\n",
    "\n",
    "#permutation, distance = solve_tsp_dynamic_programming(distances1)\n",
    "permutation, distance = solve_tsp_local_search(distances1)\n",
    "#permutation, distance = solve_tsp_dynamic_programming(distances1)\n",
    "#permutation.append(permutation[0])\n",
    "#print(distances1)\n",
    "print(permutation, distance )    \n",
    "print( best_solution, best_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2508f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62669c5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7cd1101d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relocate_algorithm: [0, 21, 11, 4, 2, 30, 36, 29, 41, 35, 8, 6, 9, 13, 37, 32, 7, 28, 40, 46, 49, 34, 20, 27, 19, 17, 26, 14, 33, 23, 15, 5, 10, 43, 48, 18, 16, 42, 22, 3, 1, 39, 47, 44, 31, 38, 24, 45, 25, 12] 643.1223730977567\n",
      "Relocated node: 49 New Distance: 637.8546487524878\n",
      "relocate_algorithm: [0, 35, 13, 26, 11, 27, 41, 25, 37, 38, 32, 40, 4, 30, 24, 28, 47, 33, 31, 17, 36, 5, 3, 12, 44, 45, 1, 34, 48, 20, 8, 7, 14, 46, 39, 9, 42, 2, 15, 23, 10, 6, 49, 43, 18, 22, 19, 21, 29, 16] 733.2090982351375\n",
      "Relocated node: 26 New Distance: 730.0415408521599\n",
      "relocate_algorithm: [0, 5, 22, 23, 28, 12, 25, 17, 33, 32, 3, 10, 13, 44, 1, 11, 37, 35, 46, 27, 38, 7, 4, 19, 31, 8, 43, 20, 48, 15, 34, 6, 16, 39, 40, 45, 41, 47, 18, 42, 9, 30, 26, 14, 49, 36, 2, 24, 29, 21] 637.8892917239688\n",
      "Relocated node: 5 New Distance: 637.1700001615925\n",
      "Relocated node: 5 New Distance: 634.1150792031218\n",
      "Relocated node: 5 New Distance: 625.8834436328647\n",
      "Relocated node: 5 New Distance: 624.3784829220099\n",
      "relocate_algorithm: [33, 36, 23, 48, 17, 28, 29, 26, 1, 25, 16, 37, 22, 40, 10, 2, 30, 14, 7, 27, 46, 42, 18, 43, 34, 13, 44, 6, 49, 21, 4, 8, 38, 45, 20, 24, 41, 32, 12, 39, 47, 15, 3, 9, 11, 35, 31, 5, 19, 0] 753.7301755837042\n",
      "Relocated node: 43 New Distance: 738.9546808655235\n",
      "Relocated node: 43 New Distance: 731.9160496316771\n",
      "Relocated node: 34 New Distance: 730.2924096606642\n",
      "relocate_algorithm: [0, 10, 12, 22, 2, 30, 35, 25, 26, 1, 31, 5, 44, 13, 18, 7, 34, 14, 39, 29, 23, 41, 43, 15, 33, 16, 47, 36, 19, 24, 6, 4, 3, 37, 32, 17, 49, 27, 28, 38, 42, 8, 21, 48, 11, 20, 45, 9, 46, 40] 643.270397886378\n",
      "Relocated node: 10 New Distance: 635.7292406382433\n",
      "relocate_algorithm: [32, 36, 31, 41, 23, 35, 49, 8, 3, 46, 18, 26, 14, 4, 19, 33, 5, 34, 25, 28, 20, 37, 24, 40, 39, 7, 42, 9, 11, 10, 44, 38, 30, 47, 17, 22, 1, 16, 29, 13, 48, 12, 27, 15, 21, 2, 43, 45, 6, 0] 695.8461808642307\n",
      "Relocated node: 17 New Distance: 693.9503625142404\n",
      "Relocated node: 22 New Distance: 687.7831045574244\n",
      "Relocated node: 47 New Distance: 686.5994172707697\n",
      "Relocated node: 47 New Distance: 681.507738516719\n",
      "relocate_algorithm: [0, 11, 7, 22, 34, 41, 42, 15, 29, 14, 32, 28, 8, 49, 12, 31, 24, 2, 44, 40, 19, 43, 10, 17, 5, 37, 4, 27, 9, 35, 38, 23, 47, 45, 33, 39, 16, 48, 26, 1, 36, 30, 20, 21, 46, 6, 13, 3, 25, 18] 696.9035034425311\n",
      "Relocated node: 31 New Distance: 691.7224576741567\n",
      "Relocated node: 24 New Distance: 691.1402057048261\n",
      "Relocated node: 24 New Distance: 687.9915903612662\n",
      "Relocated node: 24 New Distance: 687.1304136702598\n",
      "Relocated node: 24 New Distance: 678.8101357814332\n",
      "relocate_algorithm: [4, 40, 20, 2, 18, 9, 22, 15, 26, 39, 49, 19, 44, 31, 12, 5, 45, 32, 48, 30, 13, 11, 23, 8, 17, 43, 16, 25, 10, 14, 35, 36, 41, 29, 21, 38, 47, 6, 3, 7, 28, 37, 34, 33, 27, 42, 46, 1, 0, 24] 688.732191292822\n",
      "Relocated node: 40 New Distance: 688.7304838250698\n",
      "Relocated node: 40 New Distance: 688.6079324625211\n",
      "Relocated node: 40 New Distance: 688.2879008473722\n",
      "Relocated node: 20 New Distance: 687.7158191359292\n",
      "Relocated node: 40 New Distance: 687.3313037274128\n",
      "Relocated node: 20 New Distance: 683.6996081265188\n",
      "Relocated node: 20 New Distance: 682.91628511705\n",
      "Relocated node: 20 New Distance: 681.8411808277552\n",
      "Relocated node: 20 New Distance: 681.7186294652066\n",
      "Relocated node: 40 New Distance: 681.1841095017592\n",
      "Relocated node: 20 New Distance: 676.4696126888542\n",
      "Relocated node: 40 New Distance: 672.5693460450157\n",
      "relocate_algorithm: [0, 35, 19, 41, 34, 36, 28, 45, 23, 9, 1, 16, 3, 22, 18, 33, 42, 27, 15, 13, 11, 17, 7, 4, 48, 46, 29, 47, 6, 32, 20, 44, 25, 2, 12, 26, 8, 21, 24, 38, 49, 10, 31, 5, 37, 40, 30, 43, 39, 14] 723.9765862941902\n",
      "Relocated node: 31 New Distance: 722.7583813128833\n",
      "Relocated node: 31 New Distance: 722.1700964937461\n",
      "relocate_algorithm: [0, 9, 6, 8, 26, 12, 40, 1, 39, 31, 29, 17, 46, 28, 19, 44, 22, 33, 20, 13, 27, 32, 38, 21, 48, 14, 41, 3, 30, 35, 2, 7, 42, 15, 37, 47, 5, 43, 45, 11, 34, 18, 16, 49, 4, 25, 36, 23, 10, 24] 716.9277892700343\n",
      "Relocated node: 9 New Distance: 714.3858644126667\n",
      "Relocated node: 9 New Distance: 713.8188542040922\n",
      "Relocated node: 9 New Distance: 713.7992854665472\n",
      "Relocated node: 9 New Distance: 702.8975245059656\n",
      "Relocated node: 9 New Distance: 698.2811758066234\n",
      "Relocated node: 9 New Distance: 695.9063170478486\n",
      "Relocated node: 9 New Distance: 689.0806442984326\n",
      "Relocated node: 9 New Distance: 675.6837799688326\n",
      "Relocated node: 6 New Distance: 661.6992621862481\n",
      "Relocated node: 43 New Distance: 661.4861406143177\n",
      "Relocated node: 43 New Distance: 657.6690587335041\n",
      "relocate_algorithm: [0, 23, 38, 6, 3, 20, 46, 1, 17, 40, 12, 11, 36, 21, 35, 18, 47, 29, 5, 26, 33, 28, 22, 25, 49, 37, 45, 32, 43, 9, 34, 16, 31, 42, 41, 24, 4, 30, 8, 44, 13, 2, 27, 19, 14, 10, 39, 7, 48, 15] 676.0860525435985\n",
      "Relocated node: 23 New Distance: 664.3048681140614\n",
      "Relocated node: 23 New Distance: 657.7594250011306\n",
      "Relocated node: 23 New Distance: 655.3757507319372\n",
      "relocate_algorithm: [0, 49, 37, 7, 9, 42, 21, 14, 12, 6, 16, 28, 30, 38, 13, 24, 26, 39, 31, 4, 23, 47, 33, 32, 35, 27, 15, 45, 19, 10, 8, 48, 2, 1, 43, 41, 34, 5, 20, 22, 36, 3, 44, 18, 11, 17, 46, 40, 29, 25] 677.3516476292284\n",
      "Relocated node: 24 New Distance: 671.7562115995095\n",
      "Relocated node: 39 New Distance: 671.7562115995094\n",
      "Relocated node: 24 New Distance: 670.6358385703787\n",
      "Relocated node: 38 New Distance: 664.5615145466612\n",
      "relocate_algorithm: [0, 6, 1, 13, 30, 11, 24, 15, 35, 49, 12, 45, 2, 46, 20, 38, 18, 32, 22, 8, 27, 16, 29, 5, 37, 47, 34, 17, 3, 26, 14, 7, 21, 19, 28, 40, 36, 39, 23, 31, 42, 25, 48, 33, 43, 10, 9, 41, 4, 44] 762.6127887929078\n",
      "Relocated node: 6 New Distance: 743.676863206468\n",
      "Relocated node: 6 New Distance: 736.951405986579\n",
      "Relocated node: 6 New Distance: 733.3984361612069\n",
      "Relocated node: 6 New Distance: 718.7928184890792\n",
      "relocate_algorithm: [0, 42, 18, 13, 20, 49, 40, 47, 4, 16, 15, 32, 6, 12, 10, 37, 1, 38, 8, 41, 22, 14, 48, 3, 9, 23, 44, 21, 30, 11, 29, 43, 34, 46, 39, 36, 24, 27, 25, 2, 35, 33, 31, 28, 19, 7, 45, 26, 5, 17] 690.1186235290321\n",
      "Relocated node: 42 New Distance: 689.105381129113\n",
      "Relocated node: 42 New Distance: 686.6995483610024\n",
      "Relocated node: 42 New Distance: 686.3035591954822\n",
      "Relocated node: 42 New Distance: 685.6250686483403\n",
      "Relocated node: 42 New Distance: 680.1893792118643\n",
      "relocate_algorithm: [0, 4, 42, 41, 12, 30, 19, 11, 17, 2, 31, 21, 23, 38, 36, 14, 3, 15, 40, 35, 46, 20, 32, 10, 6, 27, 1, 48, 44, 22, 29, 26, 25, 24, 8, 39, 16, 5, 7, 37, 33, 28, 45, 43, 9, 34, 49, 13, 47, 18] 706.9985686366432\n",
      "Relocated node: 4 New Distance: 703.4499055575308\n",
      "Relocated node: 4 New Distance: 702.4193465237722\n",
      "Relocated node: 42 New Distance: 685.2479972006611\n",
      "Relocated node: 42 New Distance: 673.7550963424695\n",
      "Relocated node: 41 New Distance: 673.7550963424694\n",
      "644.3093867589319 637.8546487524878\n",
      "685.3827769232448 730.0415408521599\n",
      "679.921174891938 624.3784829220099\n",
      "723.8598669464598 730.2924096606642\n",
      "603.4070022402451 635.7292406382433\n",
      "707.6732164896023 681.507738516719\n",
      "693.4623281651036 678.8101357814332\n",
      "679.8266701374786 672.5693460450157\n",
      "732.5096309588032 722.1700964937461\n",
      "747.4533609139116 657.6690587335041\n",
      "697.0827462714691 655.3757507319372\n",
      "692.4909310827974 664.5615145466612\n",
      "696.0829304539105 718.7928184890792\n",
      "712.9453238940683 680.1893792118643\n",
      "696.313697815367 673.7550963424694\n",
      "tensor(1329.6263)\n",
      "Epoch [1/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [2/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [3/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [4/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [5/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [6/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [7/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [8/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [9/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [10/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [11/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [12/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [13/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [14/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [15/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [16/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [17/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [18/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [19/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [20/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [21/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [22/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [23/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [24/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [25/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [26/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [27/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [28/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [29/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [30/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [31/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [32/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [33/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [34/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [35/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [36/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [37/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [38/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [39/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [40/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [41/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [42/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [43/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [44/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [45/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [46/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [47/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [48/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [49/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [50/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [51/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [52/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [53/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [54/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [55/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [56/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [57/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [58/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [59/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [60/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [61/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [62/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [63/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [64/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [65/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [66/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [67/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [68/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [69/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [70/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [71/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [72/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [73/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [74/100], Loss: 88.6418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1329.6263)\n",
      "Epoch [75/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [76/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [77/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [78/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [79/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [80/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [81/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [82/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [83/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [84/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [85/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [86/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [87/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [88/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [89/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [90/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [91/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [92/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [93/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [94/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [95/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [96/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [97/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [98/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [99/100], Loss: 88.6418\n",
      "tensor(1329.6263)\n",
      "Epoch [100/100], Loss: 88.6418\n"
     ]
    }
   ],
   "source": [
    "#import numpy as np\n",
    "#import pandas as pd\n",
    "#import torch\n",
    "#Sto sigkekrimeno kommati tou kwdika ftiaxnoume random senaria gia kombous gia ekpaideush to TSP\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "def generate_tsp_instances(num_instances, num_nodes):\n",
    "    instances = []\n",
    "    for _ in range(num_instances):\n",
    "        nodes = np.random.rand(num_nodes, 2)\n",
    "        instances.append(nodes)\n",
    "    #edw ftiaxnonte ta tixaia senaria kai ta pros8etoume san pinaka se enan megalitero pinaka\n",
    "   \n",
    "    return instances\n",
    "\n",
    "\n",
    "#edw exoume ton ar8im8o pou 8a einai ta senaria p.x 200 senaria apo 5 diaforetikes polis\n",
    "num_instances = 15\n",
    "\n",
    "#edw dialegoume ton ari8mo ton poleon analoga me ton ari8mo tou senariou pou exoume\n",
    "num_nodes = len(coords)\n",
    "\n",
    "instances = generate_tsp_instances(num_instances, num_nodes)\n",
    "edge_index_train1 = ApoKomb1.edge_index_training(instances)\n",
    "distances_train = ApoKomb1.apostasis_training(instances)\n",
    "distances_train= torch.tensor(distances_train, dtype=torch.float32)\n",
    "\n",
    "\n",
    "#briskoume tis apostaseis se enan pinaka [n,n] \n",
    "distances_train_sq = ApoKomb1.apostasis_test_train(coords,instances)\n",
    "\n",
    "\n",
    "train_dis =[]\n",
    "edge_index_train3=[]\n",
    "\n",
    "edge_index_train2= torch.tensor(edge_index_train1, dtype=torch.long)\n",
    "\n",
    "for i in range(len(edge_index_train1)):\n",
    "    train_dis.append(torch.unsqueeze(distances_train[i], 1))\n",
    "    edge_index_train3.append(edge_index_train2[i].t().contiguous())\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "#briskoume ola ta regret gia ola ta train models    \n",
    "train_model = []\n",
    "for i in range(num_instances):\n",
    "    train_model.append(model(train_dis[i], edge_index_train3[i]))\n",
    "    train_model[i] = torch.squeeze(train_model[i], 1)\n",
    "    train_model[i]= train_model[i].detach().numpy()\n",
    "    \n",
    "\n",
    "    \n",
    "#print(edge_index_train1[0])\n",
    "\n",
    "best_dist_train=np.zeros(num_instances)    \n",
    "best_solution_train= []\n",
    "\n",
    "\n",
    "for i in range(num_instances):\n",
    "    best_solution_train,best_dist_train[i] = gls(distances_train_sq[i], train_model[i], edge_index_train1[i])\n",
    "    #permutation, distance = solve_tsp_dynamic_programming(distances1)\n",
    "    \n",
    "\n",
    "    \n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Assuming L2 loss is mean squared error\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.99)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "#print(best_dist_train)\n",
    "\n",
    "targets= np.zeros(num_instances) \n",
    "\n",
    "for i in range(num_instances):\n",
    "    permutation, targets[i] = solve_tsp_local_search(distances_train_sq[i])\n",
    "\n",
    "for i in range(num_instances):\n",
    "    print(targets[i],best_dist_train[i] )\n",
    "    \n",
    "    \n",
    "    \n",
    "best_dist_train=torch.tensor(best_dist_train, dtype=torch.float32)    \n",
    "targets=torch.tensor(targets, dtype=torch.float32)\n",
    "#print(targets)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i in range(0, len(instances), batch_size):\n",
    "        batch_instances = instances[i:i + batch_size]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        #outputs = model(inputs, edge_index1)\n",
    "        loss = criterion(best_dist_train, targets)\n",
    "        print(loss)\n",
    "        #loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(instances):.4f}\")\n",
    "\n",
    "    #print(\"Output shape:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf065c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac8d443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from python_tsp.exact import solve_tsp_dynamic_programming\n",
    "\n",
    "# Convert distances and targets to PyTorch tensors\n",
    "distances_train_sq = torch.tensor(distances_train_sq, dtype=torch.float32)\n",
    "best_dist_train = torch.tensor(best_dist_train, dtype=torch.float32)\n",
    "targets = torch.zeros(num_instances, dtype=torch.float32)\n",
    "\n",
    "for i in range(num_instances):\n",
    "    permutation, targets[i] = solve_tsp_dynamic_programming(distances_train_sq[i].numpy())\n",
    "\n",
    "# Move tensors to the appropriate device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "best_dist_train = best_dist_train.to(device)\n",
    "targets = targets.to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Assuming L2 loss is mean squared error\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.99)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i in range(0, len(instances), batch_size):\n",
    "        batch_instances = instances[i:i + batch_size]\n",
    "        \n",
    "        # Convert batch instances to PyTorch tensors and move to device\n",
    "        # Assuming batch_instances is a list of numpy arrays\n",
    "        batch_inputs = [torch.tensor(inst, dtype=torch.float32).to(device) for inst in batch_instances]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Perform forward pass (assuming your model handles batch processing)\n",
    "        batch_outputs = [model(inputs, edge_index1) for inputs in batch_inputs]  # Modify edge_index1 accordingly if it varies per instance\n",
    "        \n",
    "        # Calculate loss for each batch output\n",
    "        loss = 0\n",
    "        for j in range(len(batch_outputs)):\n",
    "            loss += criterion(batch_outputs[j], targets[i + j])\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(instances):.4f}\")\n",
    "\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a297d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "distances_train1= torch.tensor(distances_train, dtype=torch.float32)\n",
    "\n",
    "test =[]\n",
    "\n",
    "\n",
    "edge_index_test2=[]\n",
    "# Instantiate the GNN model\n",
    "#model = GNNModel(dx, dh, output_dim, num_layers)\n",
    "#print(adjacency_matrix)\n",
    "# Forward pass\n",
    "#print(edge_index_test1[i].t().contiguous() )\n",
    "\n",
    "edge_index_test1= torch.tensor(edge_index_test, dtype=torch.long)\n",
    "for i in range(len(edge_index_test1)):\n",
    "    test.append(torch.unsqueeze(distances_train1[i], 1))\n",
    "    #distances_train1[i]=  torch.unsqueeze(distances_train[i], 1)\n",
    "    edge_index_test2.append(edge_index_test1[i].t().contiguous())\n",
    "#print(edge_index_test1[1])\n",
    "#print(edge_index_test2 )\n",
    "#print(distances_train1)\n",
    "for i in range(num_instances):\n",
    "    \n",
    "    output = model(test[i], edge_index_test2[i]) #adjacency_matrix, \n",
    "    #print(\"Output shape:\", output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ea081b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as mp\n",
    "#from haversine import haversine\n",
    "\n",
    "#sto sigkekrimeno meros tou kwdika pernoume ta dedomena apo prin kai ta topo8etoume se enan algorithmo gia na \n",
    "#broume tis apostaseis apo ka8e kombo, sth synexia ta bazoume ta apotelesmata os (EDGE FEATURES) kai meta\n",
    "#Ftiaxnoume ena grafima me auta kai sth sunexia to metatrepoume se gramiko grafima gia na exoume ta (EDGE FEATURES) os (NODE FEATURES)\n",
    "\n",
    "# Example usage:\n",
    "# Create a sample graph\n",
    "G = nx.Graph()\n",
    "for i in range(len(edge_index)):\n",
    "    G.add_edge(edge_index[i][0], edge_index[i][1], weight=distances[i])\n",
    "\n",
    "def line_graph(G):\n",
    "    L = nx.line_graph(G)\n",
    "    for node in L.nodes:\n",
    "        edges = G.edges(node)\n",
    "        if edges:\n",
    "            edge_features = G.edges[node]['weight']\n",
    "            L.nodes[node]['e_f'] = edge_features\n",
    "        else:\n",
    "            L.nodes[node]['e_f'] = None\n",
    "    return L\n",
    "\n",
    "def line_graph_node_features(G):\n",
    "    node_features = []\n",
    "    for _, node_data in G.nodes(data=True):\n",
    "        node_features.append(node_data['e_f'])\n",
    "    return node_features\n",
    "\n",
    "L = line_graph(G)\n",
    "node_features1 = line_graph_node_features(L)\n",
    "\n",
    "# Create a mapping from original graph edges to line graph nodes\n",
    "edge_to_index = {edge: idx for idx, edge in enumerate(L.nodes())}\n",
    "\n",
    "# Prepare edge index for the line graph\n",
    "edge_index_t = []\n",
    "for edge in L.edges:\n",
    "    src, dst = edge\n",
    "    edge_index_t.append([edge_to_index[src], edge_to_index[dst]])\n",
    "\n",
    "# Convert edge list to a 2D tensor\n",
    "edge_index1 = torch.tensor(edge_index_t, dtype=torch.long).t().contiguous()\n",
    "print(edge_index1)\n",
    "# Convert node features to tensor\n",
    "nf = torch.tensor(node_features1, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "\n",
    "#nx.draw(G, with_labels= True)\n",
    "# Transform the graph into its line graph representation\n",
    "print('Original Graph: ',G)\n",
    "\n",
    "\n",
    "nx.draw(L, with_labels= True)\n",
    "print('Line Graph: ',L)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b0f393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import db1\n",
    "import ApoKomb1    \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dgl\n",
    "from dgl.nn.pytorch import GATConv\n",
    "from dgl import LineGraph\n",
    "\n",
    "\n",
    "# Sample city data\n",
    "citys = db1.basi()\n",
    "\n",
    "citys1 = citys.sample(5)\n",
    "lats_c = citys1[[\"lat\"]]\n",
    "lngs_c = citys1[[\"lng\"]]\n",
    "node_citys = citys1[[\"city\"]]\n",
    "\n",
    "location = citys1[[\"lat\", \"lng\"]].astype(float)\n",
    "\n",
    "coords = ApoKomb1.sindet(lats_c, lngs_c)\n",
    "edge_index = ApoKomb1.edge_index(node_citys)\n",
    "distances = ApoKomb1.apostasis(node_citys, coords)\n",
    "print(distances)\n",
    "\n",
    "# Convert edge_index to a NumPy array\n",
    "edge_index = np.array(edge_index)\n",
    "\n",
    "# Create a sample graph using DGL\n",
    "graph = dgl.graph((edge_index[:, 0], edge_index[:, 1]), num_nodes=len(node_citys))\n",
    "distances =torch.tensor(distances, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "#graph = dgl.from_scipy(distances, eweight_name='w')\n",
    "graph.edata['w'] = distances\n",
    "print(graph.edata['w'] )\n",
    "\n",
    "transform = LineGraph(backtracking=False)\n",
    "\n",
    "# Convert to line graph\n",
    "line_graph = transform(graph)\n",
    "\n",
    "print(line_graph)\n",
    "\n",
    "# Prepare node features for the line graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775e9d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import networkx as nx\n",
    "\n",
    "#edw dimiourgiete to nevroniko diktio to opoio exei eksis kommatia \n",
    "#1) To EmbeddingLayer to opoio exei os apotelesma sto forward (h0ij = Wxij + b,)\n",
    "#2) Ena Graph Attection Layer ftMHA(htij , L(G))\n",
    "#3) Ena Feed Forward Layer  ftFF(h˙t+1ij)\n",
    "#4) Ena Output Layer to opoio meta apo epeksergasia apo ola ta alla layers bgazei to regret sto forward rˆij = WhTij + b\n",
    "#5) Ola mazi ftiaxnoun to nevroniko diktuo \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_layers=1):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.W = nn.Parameter(torch.randn(input_dim, output_dim))  # Learnable weight matrix\n",
    "        self.b = nn.Parameter(torch.zeros(output_dim))  # Learnable biases\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        for _ in range(self.num_layers):\n",
    "            h = torch.matmul(h, self.W)\n",
    "            h += self.b\n",
    "        return h\n",
    "\n",
    "class GATLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_heads=8, concat=True):\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.gat = GATConv(input_dim, output_dim, heads=num_heads, concat=concat, dropout=0.6)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        #print(x)\n",
    "        x = self.gat(x, edge_index)\n",
    "        return F.elu(x)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, input_size, output_size=512):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class Output_layer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Output_layer, self).__init__()\n",
    "        self.W = nn.Parameter(torch.randn(input_dim, output_dim))  # Learnable weight matrix\n",
    "        self.b = nn.Parameter(torch.zeros(output_dim))  # Learnable biases\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0ij = torch.matmul(x, self.W)  # Assuming x has shape (batch_size, num_edges, input_dim)\n",
    "        h0ij += self.b\n",
    "        return h0ij\n",
    "    \n",
    "class GNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.embedding_layer = EmbeddingLayer(input_dim, hidden_dim)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.gat_layers = nn.ModuleList([GATLayer(hidden_dim, hidden_dim // 8, num_heads=8) for _ in range(num_layers)])\n",
    "        self.feedforward = FeedForward(hidden_dim, hidden_dim)  # Adjusted output_size\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_dim)\n",
    "        self.output_layer = Output_layer(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index1):\n",
    "        x1 = self.embedding_layer(x)\n",
    "        #print(\"After embedding layer:\", x1.shape)\n",
    "        #print(\"After embedding layer:\", edge_index1)\n",
    "        for layer in self.gat_layers:\n",
    "            x2 = layer(x1, edge_index1)\n",
    "            #print(\"After GAT layer:\", x2.shape)\n",
    "            x1 = x1 + x2\n",
    "\n",
    "        combined1 = x1\n",
    "        bn_output1 = self.batch_norm(combined1)\n",
    "       # print(\"After first batch norm:\", bn_output1.shape)\n",
    "        \n",
    "        # Apply feedforward operation\n",
    "        ff_output = self.feedforward(bn_output1)\n",
    "        #print(\"After feedforward layer:\", ff_output.shape)\n",
    "        \n",
    "        # Add the original input with the feedforward output\n",
    "        combined = bn_output1 + ff_output\n",
    "       # print(\"After adding feedforward output:\", combined.shape)\n",
    "        \n",
    "        # Apply batch normalization\n",
    "        bn_output = self.batch_norm(combined)\n",
    "        #print(\"After second batch norm:\", bn_output.shape)\n",
    "        \n",
    "        x = self.output_layer(bn_output)\n",
    "        #print(\"After output layer:\", x.shape)\n",
    "        return x\n",
    "    \n",
    "# Define model parameters\n",
    "dx = 1  # Dimension of edge features\n",
    "dh = 120  # Dimension of hidden layers\n",
    "output_dim = 1  # Output dimension\n",
    "num_layers = 5 # Number of GNN layers\n",
    "\n",
    "\n",
    "nf= torch.tensor(node_features1, dtype=torch.float32)\n",
    "#print(nf)\n",
    "nf= torch.unsqueeze(nf, 1)\n",
    "#print(nf)\n",
    "\n",
    "\n",
    "#adjacency_matrix = torch.tensor(nx.adjacency_matrix(L).todense(), dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Instantiate the GNN model\n",
    "model = GNNModel(dx, dh, output_dim, num_layers)\n",
    "#print(adjacency_matrix)\n",
    "# Forward pass\n",
    "\n",
    "#edge_index1= torch.tensor(edge_index, dtype=torch.long)\n",
    "\n",
    "\n",
    "#edge_index1 = edge_index1.t().contiguous()\n",
    "#print(edge_index1)\n",
    "output = model(graph.edata['w'] , line_graph) #adjacency_matrix, \n",
    "print(\"Output shape:\", output)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
